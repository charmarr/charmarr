name: charmarr-storage-k8s
type: charm
title: Charmarr Storage
summary: Shared media storage provider for Charmarr applications
description: |
  Manages a shared PersistentVolumeClaim (PVC) that enables hardlinks
  across all Charmarr media applications (Radarr, Sonarr, qBittorrent, Plex).

  Supports two backend modes:
  - storage-class: Use existing Kubernetes StorageClass
  - native-nfs: Manage PV/PVC for Kubernetes native NFS driver

  This is a workload-less charm - it manages K8s resources via API calls
  without running any container workload.

  Publishes PUID/PGID to all connected applications for consistent
  file ownership across the media stack.

  Note: When using native-nfs backend with MicroK8s, MicroK8s must be
  installed with --classic confinement for NFS mounts to work.

links:
  documentation: https://github.com/charmarr/charmarr
  source: https://github.com/charmarr/charmarr
  issues: https://github.com/charmarr/charmarr/issues

assumes:
  - k8s-api
  - juju >= 3.6

base: ubuntu@24.04
platforms:
  amd64:

parts:
  charm:
    source: .
    plugin: uv
    build-packages: [git]
    build-snaps: [astral-uv]
# FIXME: git describe doesn't work - charmcraft copies source without .git directory
#    override-build: |
#      craftctl default
#      git describe --always > $CRAFT_PART_INSTALL/version

provides:
  media-storage:
    interface: media-storage

config:
  options:
    backend-type:
      type: string
      default: ""
      description: |
        Storage backend type. REQUIRED.

        Options:
        - storage-class: Use existing Kubernetes StorageClass
        - native-nfs: Use Kubernetes native NFS driver (managed by charm)
        - hostpath: Use host filesystem path (single-node clusters)

        Cannot be changed after initial deployment.

    storage-class:
      type: string
      default: ""
      description: |
        Kubernetes StorageClass name to use (backend-type: storage-class).

        Examples:
        - local-path (single-node, RWO)
        - topolvm-provisioner (multi-node local storage, RWO)
        - nfs-csi (multi-node NFS, RWX)

        Use 'kubectl get storageclass' to see available options.

    size:
      type: string
      default: "100Gi"
      description: |
        Storage size to provision. Can be increased later (not decreased).

        For storage-class: Initial PVC size, expansion handled by CSI driver.
        For native-nfs: Informational only (NFS share size managed externally).

        Examples: 100Gi, 1Ti, 5Ti

    nfs-server:
      type: string
      default: ""
      description: |
        NFS server IP or hostname (backend-type: native-nfs).

        Example: 192.168.1.100 or nas.local

    nfs-path:
      type: string
      default: ""
      description: |
        NFS export path (backend-type: native-nfs).

        Example: /mnt/pool/charmarr-media

    hostpath:
      type: string
      default: ""
      description: |
        Host filesystem path (backend-type: hostpath).

        The path must exist on the node and be accessible to pods.
        Best for single-node clusters with existing local storage (ZFS, BTRFS).

        Example: /media or /mnt/storage/charmarr

    access-mode:
      type: string
      default: "ReadWriteMany"
      description: |
        PVC access mode (storage-class backend only).

        ReadWriteMany: Pods can run on any node (requires NFS or similar)
        ReadWriteOnce: All pods locked to same node (works with local storage)

        Ignored for native-nfs backend (always ReadWriteMany).

        If StorageClass doesn't support ReadWriteMany, charm will detect
        and suggest changing to ReadWriteOnce.

    puid:
      type: int
      default: 1000
      description: |
        User ID for file ownership. Published to all connected applications.

        All LinuxServer.io containers will run as this UID, ensuring
        consistent file ownership across the entire media stack.

        Default 1000 is the standard first non-root user on most Linux systems.

        For NFS backends, set this to match your NFS export's expected UID.

    pgid:
      type: int
      default: 1000
      description: |
        Group ID for file ownership. Published to all connected applications.

        All LinuxServer.io containers will use this GID, ensuring
        consistent file ownership across the entire media stack.

        Default 1000 is the standard first non-root group on most Linux systems.

        For NFS backends, set this to match your NFS export's expected GID.

    cleanup-on-remove:
      type: boolean
      default: true
      description: |
        Delete charm-managed K8s storage resources when the charm is removed.

        When false (default): Resources persist for manual cleanup or reattachment.
        When true: Resources are cleaned up on removal.

        WARNING: Setting to true may cause data loss depending on StorageClass
        reclaim policy.
